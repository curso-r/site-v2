---
title: "O pacote arrow"
date: "2019-08-31"
tags: ["arrow", "dados"]
categories: ["tutoriais", "r"]
banner: "img/banners/janitor-mov.jpg"
author: ["Julio"]
summary: "A faxina de dados é uma etapa essencial do ciclo da ciência de dados, que nunca será automatizada. Seja parte desse movimento."
draft: true
editor_options: 
  chunk_output_type: console
---

```{r, include=FALSE}
knitr::opts_chunk$set(echo=FALSE, out.width = "50%")
```

O pacote `{arrow}` é uma atualização muito importante para o mundo da ciência de dados. Essa tecnologia possibilitará uma integração ainda mais suave entre as principais linguagens de programação voltadas à ciência de dados, como `R/Python`, bem como as diversas ferramentas que utilizamos na parte de engenharia de dados, como `Spark`, por exemplo.

Cada vez mais, o que percebo é que a ciência de dados está conseguindo atingir uma meta que parece contraditória: unificar as ferramentas e, ao mesmo tempo, dar liberdade ao profissional. Isso acontece pois a parte que se quer libertar é a do pensamento crítico, a modelagem e o problema de negócio, e o que se quer padronizar é o armazenamento, o deploy e a escalabilidade.

## O que é o Arrow?

Segundo o site da Apache:

> Apache Arrow is a development platform for in-memory analytics. It contains a set of technologies that enable big data systems to process and move data fast. It specifies a standardized language-independent columnar memory format for flat and hierarchical data, organized for efficient analytic operations on modern hardware.

A instalação do `{arrow}` ainda é um pouco complicada e não convencional. Para usuários linux, como eu, é necessário baixar o código fonte do github e mandar compilar na mão. O [tutorial para isso é relativamente simples de seguir](https://arrow.apache.org/docs/developers/cpp.html), mas acredito que ainda pode melhorar.

Uma simples utilização do `{arrow}` é salvando e lendo objetos em extensões que podem ser utilizadas em diferentes linguagens. Um exemplo de extensão *parquet*, que pode ser lida em diversas linguagens, carregada pelo Google BigQuery e trabalhada em projetos envolvendo Spark.

```{r eval=FALSE}

arrow::write_parquet(mtcars, "~/Downloads/mtc.parquet")


```



Estamos caminhando para um futuro unificado!

É isso. Happy coding ;)


<!-- 1. Reprodutibilidade é essencial. -->
<!-- 2. O trade-off entre tempo investido e qualidade da base não é linear. -->
<!-- 3. A faxina é contínua. -->


